Here's an example of how you can build a sophisticated caching system in Go using the following features:

- **TTL (Time-To-Live)**: Each cache entry has an associated expiration time.
- **LRU Eviction**: The least recently used entries are evicted when the cache reaches its capacity.
- **Persistence**: Cache entries are stored on disk for persistence across restarts and node failures.
- **Distributed Cache Invalidation**: When a cache entry is invalidated on one node, it is also invalidated across all other nodes in the cluster.

Here's an example implementation:

```go
package main

import (
    "bytes"
    "context"
    "encoding/gob"
    "errors"
    "io/ioutil"
    "log"
    "path/filepath"

    "github.com/go-redis/redis/v8"
    "github.com/patrickmn/go-cache"
)

// DistributedCache represents a distributed cache system.
type DistributedCache struct {
    // Local cache for the current node
    localCache *cache.Cache

    // Persistence storage for the local cache
    persistDir string

    // Redis connection to communicate with other nodes in the cluster
    redisClient *redis.Client

    // Map of invalidated keys across all nodes in the cluster
    invalidatedKeys map[string]bool
}

// NewDistributedCache returns a new instance of DistributedCache.
func NewDistributedCache(persistDir, redisAddr string) (*DistributedCache, error) {
    localCache := cache.New(0, 10*time.Minute)
    persistDir = filepath.Join(persistDir, "cache")
    if err := os.MkdirAll(persistDir, 0755); err != nil {
        return nil, err
    }

    redisClient := redis.NewClient(&redis.Options{
        Addr: redisAddr,
    })

    invalidatedKeys := make(map[string]bool)

    dc := &DistributedCache{
        localCache:       localCache,
        persistDir:       persistDir,
        redisClient:      redisClient,
        invalidatedKeys:  invalidatedKeys,
    }

    return dc, nil
}

// Get retrieves a value from the cache.
func (dc *DistributedCache) Get(ctx context.Context, key string) (*bytes.Buffer, error) {
    val, err := dc.localCache.Get(key)
    if err != nil && !cache.IsMiss(err) {
        return nil, err
    }

    if val == nil {
        // If the entry is not in local cache and is also not invalidated,
        // try to retrieve it from other nodes in the cluster.
        redisVal, err := dc.redisClient.Get(ctx, key).Result()
        if err != nil && !redis.IsNil(err) {
            return nil, err
        }

        if redisVal == "" {
            return nil, errors.New("key not found")
        }

        // Store the retrieved value in local cache and disk persistence.
        go func() {
            dc.localCache.Set(key, &bytes.Buffer{Bytes: []byte(redisVal)}, time.Duration(dc.redisClient.TTL(key))*time.Second)
            persistFile := filepath.Join(dc.persistDir, key)
            if err := ioutil.WriteFile(persistFile, []byte(redisVal), 0644); err != nil {
                log.Printf("Error writing to file %s: %v", persistFile, err)
            }
        }()

        return &bytes.Buffer{Bytes: []byte(redisVal)}, nil
    }

    // If the entry is already in local cache and disk persistence,
    // we don't need to retrieve it from other nodes.
    return val.(*bytes.Buffer), nil
}

// Set stores a new value in the cache with TTL.
func (dc *DistributedCache) Set(ctx context.Context, key string, value *bytes.Buffer, ttl time.Duration) error {
    dc.localCache.Set(key, value, ttl)
    persistFile := filepath.Join(dc.persistDir, key)
    if err := ioutil.WriteFile(persistFile, value.Bytes(), 0644); err != nil {
        return err
    }

    // Notify all other nodes in the cluster that this cache entry is set.
    _, err := dc.redisClient.Publish(ctx, "cache-set", key).Result()
    if err != nil {
        log.Printf("Error notifying other nodes: %v", err)
    }

    return nil
}

// Invalidate removes a value from the cache and notifies all other nodes in the cluster to invalidate it as well.
func (dc *DistributedCache) Invalidate(ctx context.Context, key string) error {
    // Notify all other nodes in the cluster that this cache entry is invalidated.
    _, err := dc.redisClient.Publish(ctx, "cache-invalidated", key).Result()
    if err != nil {
        log.Printf("Error notifying other nodes: %v", err)
    }

    delete(dc.invalidatedKeys, key)

    // Remove the value from local cache and disk persistence
    dc.localCache.Delete(key)
    persistFile := filepath.Join(dc.persistDir, key)
    if err := os.Remove(persistFile); err != nil && !os.IsNotExist(err) {
        return err
    }

    return nil
}

func main() {
    // Initialize the distributed cache system.
    cacheAddr := "localhost:6379"
    persistDir := "/path/to/persistence/dir"

    dc, err := NewDistributedCache(persistDir, cacheAddr)
    if err != nil {
        log.Fatal(err)
    }

    // Use the distributed cache system
    ctx := context.Background()
    val, err := dc.Get(ctx, "my-key")
    if err != nil {
        log.Printf("Error getting key: %v", err)
    } else {
        log.Println(val.String())
    }

    // Set a new value in the cache with TTL.
    buf := bytes.NewBuffer([]byte("Hello, World!"))
    ttl := 1 * time.Hour
    if err := dc.Set(ctx, "my-key", buf, ttl); err != nil {
        log.Printf("Error setting key: %v", err)
    }

    // Invalidate a value in the cache.
    if err := dc.Invalidate(ctx, "my-key"); err != nil {
        log.Printf("Error invalidating key: %v", err)
    }
}
```

**Note**: This example code provides a basic structure for building a distributed caching system. However, it's essential to consider scalability, reliability, and performance in a production-ready implementation.

The following improvements can be made:

- **Locking Mechanism**: To ensure thread-safety when writing to the cache and disk persistence.
- **Connection Pooling**: Utilize connection pooling with Redis for better performance.
- **Scalability**: Implement load balancing across multiple nodes, consider using a distributed locking system like Redlock or ZooKeeper.

**Testing**: Write unit tests to cover the following scenarios:

*   Cache miss, hit, and eviction
*   Persistence and deserialization of cache values
*   Distributed cache invalidation and synchronization

Please remember that this is an illustrative example. Consider your specific use case and adjust the code accordingly.