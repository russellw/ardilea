Creating a concurrent web scraper in Go involves several steps, including setting up the concurrency model, implementing rate limiting, retry logic, and handling errors gracefully. Below is a simplified example to get you started.

### Prerequisites

1. **Install Go**: Make sure you have Go installed on your system. You can download it from [golang.org](https://golang.org/dl/).

2. **Set up your project**: Create a new directory for your project and initialize it with `go mod init <module-name>`.

3. **Install required packages**: For this example, we'll use the `net/http` package for HTTP requests and `time` for rate limiting.

### Implementation

Here's a basic implementation of a concurrent web scraper in Go:

```go
package main

import (
	"fmt"
	"io/ioutil"
	"net/http"
	"time"

	"golang.org/x/time/rate"
)

// ScrapeResult holds the result of a scrape operation.
type ScrapeResult struct {
	URL     string
	Data    []byte
	Err     error
}

// Scraper represents a web scraper with rate limiting and retry logic.
type Scraper struct {
	client *http.Client
	limiter *rate.Limiter
	retries int
}

// NewScraper creates a new Scraper instance.
func NewScraper(maxRPS float64, retries int) *Scraper {
	return &Scraper{
		client: &http.Client{Timeout: 10 * time.Second},
		limiter: rate.NewLimiter(rate.Every(1/time.Second), 1),
		retries: retries,
	}
}

// Scrape fetches the content of a given URL.
func (s *Scraper) Scrape(url string) (*ScrapeResult, error) {
	var resp *http.Response
	var err error

	for attempt := 0; attempt < s.retries; attempt++ {
		if !s.limiter.Allow() {
			time.Sleep(s.limiter.Reserve().Delay())
		}

		resp, err = s.client.Get(url)
		if err == nil && resp.StatusCode == http.StatusOK {
			break
		}
		fmt.Printf("Attempt %d failed for URL: %s, error: %v\n", attempt+1, url, err)
		time.Sleep(2 * time.Second) // Exponential backoff
	}

	if err != nil || resp.StatusCode != http.StatusOK {
		return &ScrapeResult{URL: url, Err: fmt.Errorf("failed to scrape URL: %s after %d attempts", url, s.retries)}, nil
	}

	defer resp.Body.Close()
	data, err := ioutil.ReadAll(resp.Body)
	if err != nil {
		return &ScrapeResult{URL: url, Err: fmt.Errorf("failed to read response body for URL: %s", url)}, nil
	}

	return &ScrapeResult{URL: url, Data: data}, nil
}

// Concurrently scrape multiple URLs.
func main() {
	urls := []string{
		"https://example.com",
		"https://another-example.com",
		// Add more URLs as needed
	}

	scraper := NewScraper(5.0, 3) // 5 requests per second with 3 retries

	resultsChan := make(chan ScrapeResult)
	for _, url := range urls {
		go func(url string) {
			result, err := scraper.Scrape(url)
			if err != nil {
				fmt.Println(err)
			} else {
				resultsChan <- *result
			}
		}(url)
	}

	closeChan := make(chan struct{})
	go func() {
		for range urls {
			select {
			case result := <-resultsChan:
				fmt.Printf("Scraped URL: %s, Data size: %d bytes\n", result.URL, len(result.Data))
			case <-closeChan:
				return
			}
		}
		close(closeChan)
	}()

	<-closeChan
}
```

### Explanation

1. **Scraper Structure**: The `Scraper` struct contains an HTTP client, a rate limiter, and the number of retries.

2. **Rate Limiting**: The rate limiter is set up using the `golang.org/x/time/rate` package to limit the number of requests per second.

3. **Retries**: The scraper attempts to fetch each URL multiple times in case of transient errors.

4. **Concurrency**: The `main` function uses goroutines to scrape URLs concurrently and sends results through a channel.

5. **Graceful Shutdown**: The program waits for all scraping tasks to complete before exiting.

This example provides a basic framework for a concurrent web scraper with rate limiting, retries, and error handling. You can expand it by adding more sophisticated error handling, logging, or data processing as needed.